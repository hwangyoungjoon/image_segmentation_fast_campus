{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.keras._impl.keras.datasets.cifar10 import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 배치를 읽어오기 위한 next_batch 함수를 정의합니다.\n",
    "def next_batch(num,data,labels):\n",
    "    \"\"\"\n",
    "    랜덤 샘플링 숫자와 라벨을 읽어줌\n",
    "    \"\"\"\n",
    "    idx=np.arange(0,len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx=idx[:num]\n",
    "    data_shuffle=[data[i] for i in idx]\n",
    "    labels_shuffle=[labels[i] for i in idx]\n",
    "    \n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn 모델을 만듭니다.\n",
    "def build_CNN_classifier(x):\n",
    "    \"\"\"\n",
    "    x:(n_examples,32,32,3) 차원을 가진 input tensor,cifar 10 데이터의 크기는 32X32크기의 rgb채널의 컬러이미지다\n",
    "    return: tuple(y,keep_prob). y는(n_examples,10)형태의 숫자(0-9)tensor이다.\n",
    "    keep_prob는 drop out을 위한 scaler placeholder이다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 입력이미지\n",
    "    x_image=x\n",
    "    \n",
    "    # 첫번쨰 합성곱- 하나의 grayscle 이미지를 64개의 특징들로 맵핑한다. \n",
    "    w_conv1=tf.Variable(tf.truncated_normal(shape=[5,5,3,64],stddev=5e-2))\n",
    "    b_conv1=tf.Variable(tf.constant(0.1,shape=[64]))\n",
    "    h_conv1=tf.nn.relu(tf.nn.conv2d(x_image,w_conv1,strides=[1,1,1,1],padding=\"SAME\")+b_conv1)  #필터의 따른 수??\n",
    "    \n",
    "    #첫번쨰 pooling\n",
    "    h_pool1=tf.nn.max_pool(h_conv1,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "    \n",
    "    #두번쨰 합성곱  32개의 특징들을 64개의 특징드로 맵핑한다. \n",
    "    w_conv2=tf.Variable(tf.truncated_normal(shape=[5,5,64,64],stddev=5e-2))\n",
    "    b_conv2=tf.Variable(tf.constant(0.1,shape=[64]))\n",
    "    h_conv2=tf.nn.relu(tf.nn.conv2d(h_pool1,w_conv2,strides=[1,1,1,1],padding=\"SAME\")+b_conv2)\n",
    "    \n",
    "    #두번째 pooling\n",
    "    h_pool2=tf.nn.max_pool(h_conv2,ksize=[1,3,3,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "    \n",
    "    #세번쨰 합성곱\n",
    "    w_conv3=tf.Variable(tf.truncated_normal(shape=[3,3,64,128],stddev=5e-2))\n",
    "    b_conv3=tf.Variable(tf.constant(0.1,shape=[128]))\n",
    "    h_conv3=tf.nn.relu(tf.nn.conv2d(h_pool2,w_conv3,strides=[1,1,1,1],padding=\"SAME\")+b_conv3)\n",
    "    \n",
    "    \n",
    "    w_conv4=tf.Variable(tf.truncated_normal(shape=[3,3,128,128],stddev=5e-2)) #왜여기부터는 컨볼루션만 하는가?\n",
    "    b_conv4=tf.Variable(tf.constant(0.1,shape=[128]))\n",
    "    h_conv4=tf.nn.relu(tf.nn.conv2d(h_conv3,w_conv4,strides=[1,1,1,1],padding=\"SAME\")+b_conv4)\n",
    "    \n",
    "    w_conv5=tf.Variable(tf.truncated_normal(shape=[3,3,128,128],stddev=5e-2))\n",
    "    b_conv5=tf.Variable(tf.constant(0.1,shape=[128]))\n",
    "    h_conv5=tf.nn.relu(tf.nn.conv2d(h_conv4,w_conv5,strides=[1,1,1,1],padding=\"SAME\")+b_conv5)\n",
    "    \n",
    "    # fully_connected layer1    2번의 downsampling 이후에 ,우리의 32X32 이미지는 8X8X128특징맵으로 변했다.\n",
    "    w_fc1=tf.Variable(tf.truncated_normal(shape=[8*8*128,384],stddev=5e-2))  #flatten 하는데 왜 저렇게 줄이나??\n",
    "    b_fc1=tf.Variable(tf.constant(0.1,shape=[384]))\n",
    "    \n",
    "    h_conv5_flat=tf.reshape(h_conv5,[-1,8*8*128])\n",
    "    h_fc1=tf.nn.relu(tf.matmul(h_conv5_flat,w_fc1)+b_fc1)\n",
    "    \n",
    "    # drop out으로 모델의 복잡도를 컨트롤한다.\n",
    "    h_fc1_drop=tf.nn.dropout(h_fc1,keep_prob)\n",
    "    \n",
    "    # 384개의 특징들을 10개의 클래스로 맵핑한다.\n",
    "    w_fc2=tf.Variable(tf.truncated_normal(shape=[384,10],stddev=5e-2))\n",
    "    b_fc2=tf.Variable(tf.constant(0.1,shape=[10]))\n",
    "    logits=tf.matmul(h_fc1,w_fc2)+b_fc2\n",
    "    y_pred=tf.nn.softmax(logits)\n",
    "    \n",
    "    return y_pred,logits\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(epoch): 0, 트레이닝 데이터 정확도: 0.070312, 손실함수: 150.651978\n",
      "반복(epoch): 100, 트레이닝 데이터 정확도: 0.250000, 손실함수: 2.171059\n",
      "반복(epoch): 200, 트레이닝 데이터 정확도: 0.351562, 손실함수: 1.969630\n",
      "반복(epoch): 300, 트레이닝 데이터 정확도: 0.445312, 손실함수: 1.706453\n",
      "반복(epoch): 400, 트레이닝 데이터 정확도: 0.367188, 손실함수: 1.723992\n",
      "반복(epoch): 500, 트레이닝 데이터 정확도: 0.484375, 손실함수: 1.556555\n"
     ]
    }
   ],
   "source": [
    "# 인풋 아웃푸스 드롭아웃 확률을 받기위한 플레이스 홀더 지정\n",
    "x=tf.placeholder(tf.float32,shape=[None,32,32,3])\n",
    "y=tf.placeholder(tf.float32,shape=[None,10])\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=load_data()\n",
    "y_train_one_hot=tf.squeeze(tf.one_hot(y_train,10),axis=1)\n",
    "y_test_one_hot=tf.squeeze(tf.one_hot(y_test,10),axis=1)\n",
    "\n",
    "\n",
    "# convolutional neural networks그래프 실행\n",
    "y_pred,logits=build_CNN_classifier(x)\n",
    "\n",
    "#loss,최적화\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits))\n",
    "train_step=tf.train.RMSPropOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "#정확도계산\n",
    "correct_prediction=tf.equal(tf.argmax(y_pred,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(1000):\n",
    "        batch=next_batch(128,x_train,y_train_one_hot.eval())\n",
    "        \n",
    "        if i%100==0:\n",
    "            train_accuracy=accuracy.eval(feed_dict={x:batch[0],y:batch[1],keep_prob:1.0})\n",
    "            loss_print=loss.eval(feed_dict={x:batch[0],y:batch[1],keep_prob:1.0})\n",
    "            \n",
    "            print(\"반복(epoch): %d, 트레이닝 데이터 정확도: %f, 손실함수: %f\" %(i,train_accuracy,loss_print))\n",
    "#             print(\"반복(Epoch): %d, 트레이닝 데이터 정확도: %f, 손실 함수(loss): %f\" % (i, train_accuracy, loss_print))\n",
    "        sess.run(train_step,feed_dict={x:batch[0],y:batch[1],keep_prob:0.8})\n",
    "        \n",
    "    test_batch=next_batch(100000,x_test,y_test_one_hot.eval())\n",
    "    print(\"테스트 정확도: %f\" %(accuracy.eval(feed_dict={x:test_batch[0],y:test_batch[1],keep_prob:1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
